{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-RLh0m6W1O1"
      },
      "source": [
        "# **Agentic Recommendation System (POC)**\n",
        "### Hybrid Collaborative Filtering + Semantic Search with Critic-Loop Self-Correction\n",
        "\n",
        "This project implements an **Agentic Workgraph** for movie recommendations. Unlike traditional recommenders that use static formulas, this system uses a \"Critic\" agent to identify genre drift and popularity bias, automatically adjusting retrieval weights in real-time.\n",
        "\n",
        "## System Architecture\n",
        "The pipeline is orchestrated via a **StateGraph**:\n",
        "1. **Intent Agent**: Decodes user natural language into retrieval goals.\n",
        "2. **Planner Agent**: Sets the initial $w_{cf}$ (Collaborative) and $w_{sem}$ (Semantic) weights.\n",
        "3. **Retrieval Tooling**:\n",
        "   - `SimpleCFRecommender`: Item-Item similarity based on MovieLens 100k.\n",
        "   - `Semantic Index`: FAISS-based vector search for \"vibe\" and \"context.\"\n",
        "4. **Ranker v2 (Advantage-Weighted)**: Penalizes popularity bias to surface niche \"hidden gems.\"\n",
        "5. **Critic Agent**: Checks for failures (e.g., suggesting \"cheesy\" movies when user asked for \"real\").\n",
        "\n",
        "## Modular Components\n",
        "- `orchestrator.py`: The LangGraph controller.\n",
        "- `ranker_v2.py`: Implementation of Advantage-Weighted retrieval.\n",
        "- `tools/cf_tool.py`: Scalable item-item similarity engine.\n",
        "- `config.py`: Centralized hyperparameters (Novelty Lambda, Advantage Alpha).\n",
        "\n",
        "## Performance Insights\n",
        "This system successfully solves the **\"Similarity Drift\"** problem. By utilizing a 50/50 hybrid weight, the system provides a balanced list that respects both the user's specific text query and the historical behavior of similar users.\n",
        "\n",
        "## Getting Started\n",
        "1. Install requirements: `pip install pandas numpy torch langgraph openai`\n",
        "2. Configure your `OPENAI_API_KEY` in environment variables.\n",
        "3. Run the orchestrator script to see the agent trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKAN2fPqbik1"
      },
      "outputs": [],
      "source": [
        "!pip install hypercorn requests --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p agents tools graph data embeddings api rankers notebooks\n",
        "!touch agents/__init__.py tools/__init__.py graph/__init__.py api/__init__.py"
      ],
      "metadata": {
        "id": "GtKqbM4nnAeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEIt4RRvW4Bc"
      },
      "outputs": [],
      "source": [
        "# 1. Install specific versions to avoid NumPy 2.0 breaking SAR\n",
        "!pip install -q --force-reinstall numpy==1.26.4 scipy==1.11.4 pandas==2.1.4\n",
        "\n",
        "!pip install -q \\\n",
        "  recommenders \\\n",
        "  scikit-learn tqdm requests \\\n",
        "  langgraph langchain langchain-community langchain-core \\\n",
        "  transformers accelerate bitsandbytes sentencepiece \\\n",
        "  sentence-transformers \\\n",
        "  hnswlib\n",
        "\n",
        "# 2. Print versions to verify\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "print(f\"Current NumPy: {np.__version__} (Should be 1.26.4)\")\n",
        "print(f\"Current Pandas: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5shkQBsAhdbh"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "OPENAI_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not OPENAI_KEY:\n",
        "    raise RuntimeError(\n",
        "        \"OPENAI_API_KEY not found. \"\n",
        "        \"Check Colab → Secrets → OPENAI_API_KEY and restart runtime.\"\n",
        "    )\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_KEY\n",
        "print(\" OpenAI API key loaded from Colab secrets\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.py\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class POCConfig:\n",
        "    # -----------------------------\n",
        "    # Data\n",
        "    # -----------------------------\n",
        "    data_dir: str = \"data/ml-latest-small\"\n",
        "    movielens_url: str = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
        "\n",
        "    # Embeddings (persisted)\n",
        "\n",
        "    embedding_model_id: str = \"text-embedding-3-small\"\n",
        "    embedding_dim: int = 1536\n",
        "    embedding_path: str = \"embeddings/movie_embeddings.npy\"\n",
        "    embedding_ids_path: str = \"embeddings/movie_ids.json\"\n",
        "\n",
        "    # Retrieval sizes\n",
        "\n",
        "    cf_k: int = 200\n",
        "    semantic_k: int = 80\n",
        "    final_k: int = 20\n",
        "\n",
        "    llm_model_id: str = \"microsoft/phi-2\"\n",
        "    llm_max_new_tokens: int = 160\n",
        "    llm_temperature: float = 0.2\n",
        "\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Verbosity\n",
        "\n",
        "    verbose: bool = True\n",
        "    verbose_prompts: bool = False\n",
        "    verbose_openai: bool = True\n",
        "\n",
        "    # Strategy mode\n",
        "\n",
        "    # v1 = baseline hybrid\n",
        "    # v2 = agentic + critic + advantage-weighted\n",
        "    strategy_version: str = \"v2\"\n",
        "\n",
        "    # Planner constraints\n",
        "\n",
        "    enforce_hybrid_for_search: bool = True\n",
        "\n",
        "    # CF should never dominate semantic intent for search queries\n",
        "    min_cf_weight: float = 0.20\n",
        "    max_cf_weight: float = 0.60\n",
        "\n",
        "    # Advantage-Weighted Ranking\n",
        "    # How strongly we favor \"advantaged\" items over baseline popularity\n",
        "    advantage_alpha: float = 0.6\n",
        "\n",
        "    # Penalizes popularity bias (higher = more niche)\n",
        "    novelty_lambda: float = 0.20\n",
        "\n",
        "    critic_topn: int = 10\n",
        "\n",
        "    # Popularity control\n",
        "    popularity_mean_threshold: float = 0.65\n",
        "\n",
        "    # Minimum unique genres in top-N\n",
        "    genre_diversity_min_unique: int = 4\n",
        "\n",
        "    min_primary_genre_ratio: float = 0.40\n",
        "\n",
        "    # Amount to reduce CF when genre drift is detected\n",
        "    genre_drift_cf_penalty: float = -0.30\n",
        "\n",
        "    # Corresponding semantic boost\n",
        "    genre_drift_semantic_boost: float = 0.30\n",
        "\n",
        "    prefer_weight_adjustments: bool = True\n",
        "\n",
        "    allow_hard_removal_without_weight_shift: bool = False\n"
      ],
      "metadata": {
        "id": "VS_AQrRuppxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tools/item_stats.py\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ItemStats:\n",
        "    popularity_norm: Dict[int, float]\n",
        "    avg_rating_norm: Dict[int, float]\n",
        "    rating_count: Dict[int, int]\n",
        "\n",
        "    @staticmethod\n",
        "    def build(ratings: pd.DataFrame) -> \"ItemStats\":\n",
        "        # counts\n",
        "        counts = ratings.groupby(\"movieId\")[\"rating\"].count().astype(int)\n",
        "        max_c = int(counts.max()) if len(counts) else 1\n",
        "        popularity_norm = {int(mid): float(c / max_c) for mid, c in counts.items()}\n",
        "\n",
        "        # avg rating\n",
        "        avgs = ratings.groupby(\"movieId\")[\"rating\"].mean()\n",
        "        # Normalize ratings from [0.5..5] -> [0..1] (MovieLens uses 0.5 increments)\n",
        "        avg_rating_norm = {int(mid): float((r - 0.5) / (5.0 - 0.5)) for mid, r in avgs.items()}\n",
        "\n",
        "        rating_count = {int(mid): int(c) for mid, c in counts.items()}\n",
        "        return ItemStats(popularity_norm=popularity_norm, avg_rating_norm=avg_rating_norm, rating_count=rating_count)\n",
        "\n",
        "    def get_popularity(self, movie_id: int) -> float:\n",
        "        return float(self.popularity_norm.get(int(movie_id), 0.0))\n",
        "\n",
        "    def get_avg_rating(self, movie_id: int) -> float:\n",
        "        return float(self.avg_rating_norm.get(int(movie_id), 0.5))\n",
        "\n",
        "    def as_debug_dict(self, movie_id: int) -> Dict[str, Any]:\n",
        "        mid = int(movie_id)\n",
        "        return {\n",
        "            \"movieId\": mid,\n",
        "            \"popularity_norm\": self.get_popularity(mid),\n",
        "            \"avg_rating_norm\": self.get_avg_rating(mid),\n",
        "            \"rating_count\": int(self.rating_count.get(mid, 0)),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "MT71b_kOA5Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tools/data_loader.py\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from typing import Tuple\n",
        "\n",
        "import pandas as pd\n",
        "from config import POCConfig\n",
        "\n",
        "\n",
        "class MovieLensLoader:\n",
        "    \"\"\"\n",
        "    Responsible for loading the MovieLens dataset in a safe, repeatable way.\n",
        "\n",
        "    Design principles:\n",
        "    - Download only if data is missing\n",
        "    - Never re-download unnecessarily\n",
        "    - Fail loudly if data is corrupted\n",
        "    - Return clean, ready-to-use DataFrames\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: POCConfig):\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def _dataset_exists(self) -> bool:\n",
        "        \"\"\"\n",
        "        Check whether the expected MovieLens files already exist.\n",
        "        \"\"\"\n",
        "        ratings_path = os.path.join(self.cfg.data_dir, \"ratings.csv\")\n",
        "        movies_path = os.path.join(self.cfg.data_dir, \"movies.csv\")\n",
        "        return os.path.exists(ratings_path) and os.path.exists(movies_path)\n",
        "\n",
        "    def _download_and_extract(self) -> None:\n",
        "        \"\"\"\n",
        "        Download and extract the MovieLens dataset.\n",
        "        \"\"\"\n",
        "        print(\"MovieLens dataset not found. Downloading...\")\n",
        "\n",
        "        response = requests.get(self.cfg.movielens_url, timeout=120)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
        "            z.extractall(\"data\")\n",
        "\n",
        "        print(\"Download and extraction complete.\")\n",
        "\n",
        "    def load(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Load ratings and movies DataFrames.\n",
        "\n",
        "        This method is safe to call multiple times.\n",
        "        \"\"\"\n",
        "        if not self._dataset_exists():\n",
        "            self._download_and_extract()\n",
        "        else:\n",
        "            print(\"MovieLens dataset found locally. Skipping download.\")\n",
        "\n",
        "        ratings_path = os.path.join(self.cfg.data_dir, \"ratings.csv\")\n",
        "        movies_path = os.path.join(self.cfg.data_dir, \"movies.csv\")\n",
        "\n",
        "        try:\n",
        "            ratings = pd.read_csv(ratings_path)\n",
        "            movies = pd.read_csv(movies_path)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\n",
        "                \"Failed to load MovieLens CSV files. \"\n",
        "                \"The dataset may be corrupted.\"\n",
        "            ) from e\n",
        "\n",
        "        required_rating_cols = {\"userId\", \"movieId\"}\n",
        "        required_movie_cols = {\"movieId\", \"title\"}\n",
        "\n",
        "        if not required_rating_cols.issubset(ratings.columns):\n",
        "            raise ValueError(\"ratings.csv is missing required columns\")\n",
        "\n",
        "        if not required_movie_cols.issubset(movies.columns):\n",
        "            raise ValueError(\"movies.csv is missing required columns\")\n",
        "\n",
        "        return ratings, movies\n"
      ],
      "metadata": {
        "id": "1x4zRsohp8VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rankers/ranker_v1.py\n",
        "from __future__ import annotations\n",
        "from typing import Any, Dict, List\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class RankerV1:\n",
        "    \"\"\"\n",
        "    Baseline v1 ranker:\n",
        "    - linear fusion of normalized CF and semantic scores\n",
        "    - no advantage weighting\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg, movie_map: Dict[int, Dict[str, str]]):\n",
        "        self.cfg = cfg\n",
        "        self.movie_map = movie_map\n",
        "\n",
        "    def __call__(self, state: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "        cf = state.get(\"cf\", pd.DataFrame())\n",
        "        sem = pd.DataFrame(state.get(\"sem\", []))\n",
        "\n",
        "        if cf is None or isinstance(cf, list):\n",
        "            cf = pd.DataFrame(cf)\n",
        "        if sem is None:\n",
        "            sem = pd.DataFrame()\n",
        "\n",
        "        if cf.empty and sem.empty:\n",
        "            return []\n",
        "\n",
        "        if not cf.empty and not sem.empty:\n",
        "            df = cf.merge(sem, on=\"movieId\", how=\"outer\").fillna(0.0)\n",
        "        elif not cf.empty:\n",
        "            df = cf.assign(semantic_score=0.0)\n",
        "        else:\n",
        "            df = sem.assign(cf_score=0.0)\n",
        "\n",
        "        for col in [\"cf_score\", \"semantic_score\"]:\n",
        "            mx = float(df[col].max()) if len(df) else 0.0\n",
        "            if mx > 0:\n",
        "                df[col] = df[col] / mx\n",
        "\n",
        "        w_cf = float(state[\"plan\"].get(\"weight_cf\", 0.5))\n",
        "        w_sem = float(state[\"plan\"].get(\"weight_semantic\", 0.5))\n",
        "        df[\"score\"] = w_cf * df[\"cf_score\"] + w_sem * df[\"semantic_score\"]\n",
        "\n",
        "        df = df.sort_values(\"score\", ascending=False).head(self.cfg.final_k)\n",
        "\n",
        "        recs: List[Dict[str, Any]] = []\n",
        "        for r in df.itertuples(index=False):\n",
        "            mid = int(r.movieId)\n",
        "            meta = self.movie_map.get(mid, {\"title\": \"Unknown\", \"genres\": \"\"})\n",
        "            recs.append(\n",
        "                {\n",
        "                    \"movieId\": mid,\n",
        "                    \"title\": meta.get(\"title\", \"Unknown\"),\n",
        "                    \"genres\": meta.get(\"genres\", \"\"),\n",
        "                    \"score\": float(r.score),\n",
        "                    \"signals\": {\n",
        "                        \"cf\": float(getattr(r, \"cf_score\", 0.0)),\n",
        "                        \"semantic\": float(getattr(r, \"semantic_score\", 0.0)),\n",
        "                    },\n",
        "                }\n",
        "            )\n",
        "        return recs\n"
      ],
      "metadata": {
        "id": "OCKI8kYGA9Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rankers/ranker_v2.py\n",
        "from __future__ import annotations\n",
        "from typing import Any, Dict, List\n",
        "import pandas as pd\n",
        "from tools.item_stats import ItemStats\n",
        "\n",
        "\n",
        "class RankerV2:\n",
        "    \"\"\"\n",
        "    Ranker v2: Advantage-Weighted Collaborative Retrieval (POC approximation)\n",
        "\n",
        "    Steps:\n",
        "    1) Build candidate pool from CF + semantic (already in state)\n",
        "    2) Normalize signals\n",
        "    3) Compute utility = w_cf*cf + w_sem*semantic\n",
        "    4) baseline = popularity_norm (proxy for \"expected utility\")\n",
        "    5) advantage = utility - advantage_alpha * baseline\n",
        "    6) novelty_boost = novelty_lambda * (1 - popularity_norm)\n",
        "    7) final = advantage + novelty_boost\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg, movie_map: Dict[int, Dict[str, str]], item_stats: ItemStats):\n",
        "        self.cfg = cfg\n",
        "        self.movie_map = movie_map\n",
        "        self.item_stats = item_stats\n",
        "\n",
        "    def __call__(self, state: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "        cf = state.get(\"cf\", pd.DataFrame())\n",
        "        sem = pd.DataFrame(state.get(\"sem\", []))\n",
        "\n",
        "        if cf is None or isinstance(cf, list):\n",
        "            cf = pd.DataFrame(cf)\n",
        "        if sem is None:\n",
        "            sem = pd.DataFrame()\n",
        "\n",
        "        if cf.empty and sem.empty:\n",
        "            return []\n",
        "\n",
        "        if not cf.empty and not sem.empty:\n",
        "            df = cf.merge(sem, on=\"movieId\", how=\"outer\").fillna(0.0)\n",
        "        elif not cf.empty:\n",
        "            df = cf.assign(semantic_score=0.0)\n",
        "        else:\n",
        "            df = sem.assign(cf_score=0.0)\n",
        "\n",
        "        # normalize signals\n",
        "        for col in [\"cf_score\", \"semantic_score\"]:\n",
        "            mx = float(df[col].max()) if len(df) else 0.0\n",
        "            if mx > 0:\n",
        "                df[col] = df[col] / mx\n",
        "\n",
        "        plan = state.get(\"plan\", {}) or {}\n",
        "        w_cf = float(plan.get(\"weight_cf\", 0.4))\n",
        "        w_sem = float(plan.get(\"weight_semantic\", 0.6))\n",
        "\n",
        "        # utility = hybrid relevance / preference signal\n",
        "        df[\"utility\"] = w_cf * df[\"cf_score\"] + w_sem * df[\"semantic_score\"]\n",
        "\n",
        "        # baseline = popularity_norm proxy (expected utility)\n",
        "        df[\"baseline\"] = df[\"movieId\"].apply(lambda mid: self.item_stats.get_popularity(int(mid)))\n",
        "\n",
        "        # advantage + novelty\n",
        "        alpha = float(self.cfg.advantage_alpha)\n",
        "        novelty_lambda = float(self.cfg.novelty_lambda)\n",
        "\n",
        "        df[\"advantage\"] = df[\"utility\"] - alpha * df[\"baseline\"]\n",
        "        df[\"novelty_boost\"] = novelty_lambda * (1.0 - df[\"baseline\"])\n",
        "        df[\"score\"] = df[\"advantage\"] + df[\"novelty_boost\"]\n",
        "\n",
        "        df = df.sort_values(\"score\", ascending=False).head(self.cfg.final_k)\n",
        "\n",
        "        recs: List[Dict[str, Any]] = []\n",
        "        for r in df.itertuples(index=False):\n",
        "            mid = int(r.movieId)\n",
        "            meta = self.movie_map.get(mid, {\"title\": \"Unknown\", \"genres\": \"\"})\n",
        "            recs.append(\n",
        "                {\n",
        "                    \"movieId\": mid,\n",
        "                    \"title\": meta.get(\"title\", \"Unknown\"),\n",
        "                    \"genres\": meta.get(\"genres\", \"\"),\n",
        "                    \"score\": float(r.score),\n",
        "                    \"signals\": {\n",
        "                        \"cf\": float(getattr(r, \"cf_score\", 0.0)),\n",
        "                        \"semantic\": float(getattr(r, \"semantic_score\", 0.0)),\n",
        "                        \"utility\": float(getattr(r, \"utility\", 0.0)),\n",
        "                        \"baseline_popularity\": float(getattr(r, \"baseline\", 0.0)),\n",
        "                        \"advantage\": float(getattr(r, \"advantage\", 0.0)),\n",
        "                        \"novelty_boost\": float(getattr(r, \"novelty_boost\", 0.0)),\n",
        "                    },\n",
        "                }\n",
        "            )\n",
        "        return recs\n"
      ],
      "metadata": {
        "id": "6JkjhitZBBvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tools/cf_tool.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, Optional\n",
        "\n",
        "\n",
        "class SimpleCFRecommender:\n",
        "    \"\"\"\n",
        "    Deterministic item-item collaborative filtering recommender.\n",
        "\n",
        "    Design goals:\n",
        "    - Stable across runs\n",
        "    - Fully explainable\n",
        "    - No stochastic components\n",
        "    - Safe fallbacks for unknown users\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.users = None\n",
        "        self.items = None\n",
        "        self.user_idx: Dict[int, int] = {}\n",
        "        self.item_idx: Dict[int, int] = {}\n",
        "        self.idx_item: Dict[int, int] = {}\n",
        "        self.sim: Optional[np.ndarray] = None\n",
        "        self.mat: Optional[np.ndarray] = None\n",
        "\n",
        "    def fit(self, ratings: pd.DataFrame) -> None:\n",
        "        \"\"\"\n",
        "        Build an implicit user–item interaction matrix and\n",
        "        compute item–item cosine similarity.\n",
        "        \"\"\"\n",
        "        self.users = ratings[\"userId\"].unique()\n",
        "        self.items = ratings[\"movieId\"].unique()\n",
        "\n",
        "        self.user_idx = {int(u): i for i, u in enumerate(self.users)}\n",
        "        self.item_idx = {int(m): i for i, m in enumerate(self.items)}\n",
        "        self.idx_item = {i: m for m, i in self.item_idx.items()}\n",
        "\n",
        "        interaction_matrix = np.zeros(\n",
        "            (len(self.users), len(self.items)),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        for row in ratings.itertuples(index=False):\n",
        "            u = int(row.userId)\n",
        "            m = int(row.movieId)\n",
        "            interaction_matrix[self.user_idx[u], self.item_idx[m]] = 1.0\n",
        "\n",
        "        item_matrix = interaction_matrix.T\n",
        "        norms = np.linalg.norm(item_matrix, axis=1, keepdims=True) + 1e-8\n",
        "        item_matrix = item_matrix / norms\n",
        "\n",
        "        self.sim = item_matrix @ item_matrix.T\n",
        "        self.mat = interaction_matrix\n",
        "\n",
        "    def recommend(self, user_id: int, k: int = 50) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Recommend items for a user based on item–item similarity.\n",
        "\n",
        "        Always returns a valid DataFrame.\n",
        "        \"\"\"\n",
        "        if self.mat is None or self.sim is None:\n",
        "            raise RuntimeError(\"CF model not fitted. Call fit() first.\")\n",
        "\n",
        "        if user_id not in self.user_idx:\n",
        "            return pd.DataFrame(columns=[\"movieId\", \"cf_score\"])\n",
        "\n",
        "        user_vector = self.mat[self.user_idx[user_id]]\n",
        "        scores = self.sim @ user_vector\n",
        "\n",
        "        top_indices = np.argsort(-scores)[:k]\n",
        "\n",
        "        return pd.DataFrame({\n",
        "            \"movieId\": [int(self.idx_item[i]) for i in top_indices],\n",
        "            \"cf_score\": scores[top_indices].astype(float)\n",
        "        })\n"
      ],
      "metadata": {
        "id": "_Zard_G5qBh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tools/semantic_tool.py\n",
        "import os, json\n",
        "import numpy as np\n",
        "import hnswlib\n",
        "from openai import OpenAI\n",
        "from config import POCConfig\n",
        "\n",
        "class SemanticSearchTool:\n",
        "    def __init__(self, cfg: POCConfig):\n",
        "        self.cfg = cfg\n",
        "        self.client = OpenAI()\n",
        "        self.index = None\n",
        "\n",
        "    def _embed(self, texts):\n",
        "        r = self.client.embeddings.create(\n",
        "            model=self.cfg.embedding_model_id,\n",
        "            input=texts\n",
        "        )\n",
        "        X = np.array([d.embedding for d in r.data], dtype=np.float32)\n",
        "        X /= np.linalg.norm(X, axis=1, keepdims=True) + 1e-8\n",
        "        return X\n",
        "\n",
        "    def build_or_load(self, movies):\n",
        "        os.makedirs(\"embeddings\", exist_ok=True)\n",
        "\n",
        "        if os.path.exists(self.cfg.embedding_path):\n",
        "            print(\"Loading cached embeddings\")\n",
        "            X = np.load(self.cfg.embedding_path)\n",
        "            self.movie_ids = json.load(open(self.cfg.embedding_ids_path))\n",
        "        else:\n",
        "            print(\"Building embeddings (one-time)\")\n",
        "            texts = (movies.title + \" | \" + movies.genres).fillna(\"\").tolist()\n",
        "            X = np.vstack([self._embed(texts[i:i+100]) for i in range(0, len(texts), 100)])\n",
        "            np.save(self.cfg.embedding_path, X)\n",
        "            self.movie_ids = movies.movieId.astype(int).tolist()\n",
        "            json.dump(self.movie_ids, open(self.cfg.embedding_ids_path, \"w\"))\n",
        "\n",
        "        self.index = hnswlib.Index(space=\"cosine\", dim=X.shape[1])\n",
        "        self.index.init_index(len(X), ef_construction=200, M=16)\n",
        "        self.index.add_items(X, np.arange(len(X)))\n",
        "        self.index.set_ef(64)\n",
        "\n",
        "    def search(self, query, k):\n",
        "        qv = self._embed([query])\n",
        "        labels, dist = self.index.knn_query(qv, k=k)\n",
        "        return [\n",
        "            {\"movieId\": self.movie_ids[i], \"semantic_score\": 1.0 - d}\n",
        "            for i, d in zip(labels[0], dist[0])\n",
        "        ]\n"
      ],
      "metadata": {
        "id": "BhW6pOPVqHbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile agents/intent_agent.py\n",
        "import json\n",
        "from typing import Any, Dict\n",
        "\n",
        "from agents.openai_client import OpenAIJSONClient\n",
        "\n",
        "\n",
        "class IntentAgent:\n",
        "    \"\"\"\n",
        "    OpenAI-backed intent and clarification agent.\n",
        "\n",
        "    IMPORTANT:\n",
        "    - Does NOT accept an external llm object (prevents JSON serialization issues).\n",
        "    - Always returns JSON-safe primitives.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
        "        self.llm = OpenAIJSONClient(model=model, temperature=0.2)\n",
        "\n",
        "    def run(self, context: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        system_prompt = \"\"\"\n",
        "You are an intent classification agent for a movie recommender.\n",
        "\n",
        "You must follow these rules:\n",
        "- Think privately; do NOT reveal chain-of-thought.\n",
        "- Output ONLY valid JSON (no markdown, no commentary).\n",
        "- Use only the provided context. Do not invent user preferences.\n",
        "\n",
        "Your job:\n",
        "1) classify the user’s intent\n",
        "2) decide if a clarification question is needed\n",
        "3) produce a short trace list with high-level reasons (no hidden reasoning)\n",
        "\"\"\"\n",
        "\n",
        "        user_prompt = f\"\"\"\n",
        "Context (JSON):\n",
        "{json.dumps(context, indent=2)}\n",
        "\n",
        "Allowed intents (choose exactly one):\n",
        "- \"search\": user has a concrete query phrase or specific attributes\n",
        "- \"explore\": browsing / discovery\n",
        "- \"comfort\": safer, familiar picks; low novelty tolerance\n",
        "- \"quick_watch\": time-constrained; prefers shorter / low-commitment\n",
        "\n",
        "Hard rules:\n",
        "- If \"query\" is a non-empty string -> intent MUST be \"search\"\n",
        "- If novelty_tolerance exists and < 0.3 -> intent MUST be \"comfort\" (unless query is present)\n",
        "- If available_minutes exists and <= 45 -> intent SHOULD be \"quick_watch\" (unless query is present)\n",
        "\n",
        "Clarification rules:\n",
        "- Set needs_clarification=true ONLY if information is insufficient to satisfy the intent.\n",
        "- If needs_clarification=true, provide exactly ONE question.\n",
        "\n",
        "Return ONLY this JSON schema:\n",
        "{{\n",
        "  \"intent\": \"search|explore|comfort|quick_watch\",\n",
        "  \"confidence\": 0.0,\n",
        "  \"needs_clarification\": false,\n",
        "  \"clarification_question\": \"\",\n",
        "  \"trace\": [\"reason_a\", \"reason_b\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "        schema_hint = {\n",
        "            \"intent\": \"search\",\n",
        "            \"confidence\": 0.8,\n",
        "            \"needs_clarification\": False,\n",
        "            \"clarification_question\": \"\",\n",
        "            \"trace\": [\"...\"],\n",
        "        }\n",
        "\n",
        "        resp = self.llm.generate_json(system_prompt, user_prompt, force_json=True, schema_hint=schema_hint, max_retries=1)\n",
        "\n",
        "        data = resp[\"data\"] if resp[\"ok\"] else {}\n",
        "        trace = list(data.get(\"trace\", [])) if isinstance(data.get(\"trace\", []), list) else []\n",
        "        trace += resp.get(\"trace\", [])\n",
        "        trace.append(\"intent_agent_openai\")\n",
        "\n",
        "        # --- HARD GUARDS (deterministic enforcement) ---\n",
        "        query = (context.get(\"query\") or \"\").strip()\n",
        "        novelty = context.get(\"novelty_tolerance\", None)\n",
        "        minutes = context.get(\"available_minutes\", None)\n",
        "\n",
        "        intent = str(data.get(\"intent\", \"explore\"))\n",
        "        if query:\n",
        "            intent = \"search\"\n",
        "        else:\n",
        "            if isinstance(novelty, (int, float)) and novelty < 0.3:\n",
        "                intent = \"comfort\"\n",
        "            elif isinstance(minutes, (int, float)) and minutes <= 45:\n",
        "                intent = \"quick_watch\"\n",
        "\n",
        "        try:\n",
        "            confidence = float(data.get(\"confidence\", 0.6))\n",
        "        except Exception:\n",
        "            confidence = 0.6\n",
        "        confidence = max(0.0, min(1.0, confidence))\n",
        "\n",
        "        needs_clarification = bool(data.get(\"needs_clarification\", False))\n",
        "        clarification_question = str(data.get(\"clarification_question\", \"\") or \"\")\n",
        "\n",
        "        # If low confidence, ask a single deterministic clarification question\n",
        "        if confidence < 0.5 and not needs_clarification:\n",
        "            needs_clarification = True\n",
        "\n",
        "        if needs_clarification and not clarification_question:\n",
        "            # One question that helps both semantic + taste alignment\n",
        "            clarification_question = (\n",
        "                \"Do you want the sci-fi to lean more cerebral and philosophical, \"\n",
        "                \"or more emotional and character-driven?\"\n",
        "            )\n",
        "            trace.append(\"default_clarification_applied\")\n",
        "\n",
        "        return {\n",
        "            \"intent\": intent,\n",
        "            \"confidence\": confidence,\n",
        "            \"needs_clarification\": needs_clarification,\n",
        "            \"clarification_question\": clarification_question,\n",
        "            \"trace\": trace,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "2zyCYPivqJX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile agents/planner_agent.py\n",
        "from __future__ import annotations\n",
        "from typing import Any, Dict\n",
        "from config import POCConfig\n",
        "from agents.openai_client import OpenAIJSONClient\n",
        "\n",
        "\n",
        "class PlannerAgent:\n",
        "    \"\"\"\n",
        "    Planner v2 (research-aligned):\n",
        "    - For search intent, do NOT disable CF if cfg.enforce_hybrid_for_search\n",
        "    - Enforce weight_cf >= cfg.min_cf_weight when hybrid is enabled\n",
        "    - Maintain strict schema and stable defaults\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: POCConfig):\n",
        "        self.cfg = cfg\n",
        "        self.client = OpenAIJSONClient(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0.2,\n",
        "            verbose=bool(cfg.verbose_openai),\n",
        "        )\n",
        "        self.verbose = bool(cfg.verbose)\n",
        "\n",
        "    def _log(self, msg: str):\n",
        "        if self.verbose:\n",
        "            print(f\"[PlannerAgent] {msg}\")\n",
        "\n",
        "    def run(self, intent: str, context: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        intent = (intent or \"explore\").strip().lower()\n",
        "        query = (context.get(\"query\") or \"\").strip()\n",
        "\n",
        "        system_prompt = (\n",
        "            \"You are a Netflix-style Strategy Planner for a movie recommender.\\n\"\n",
        "            \"You must output ONLY valid JSON.\\n\"\n",
        "            \"Do NOT include commentary.\\n\"\n",
        "            \"Do NOT reveal hidden reasoning.\\n\"\n",
        "        )\n",
        "\n",
        "        # Research rules encoded as explicit constraints:\n",
        "        # When a query exists, semantic must be used.\n",
        "        # When hybrid enforcement is enabled, CF must be non-zero even for search\n",
        "        user_prompt = (\n",
        "            f\"Intent: {intent}\\n\"\n",
        "            f\"Query: {query}\\n\"\n",
        "            f\"Context JSON:\\n{context}\\n\\n\"\n",
        "            \"Return JSON with exactly these keys:\\n\"\n",
        "            \"{\\n\"\n",
        "            '  \"use_cf\": boolean,\\n'\n",
        "            '  \"use_semantic\": boolean,\\n'\n",
        "            '  \"weight_cf\": number (0..1),\\n'\n",
        "            '  \"weight_semantic\": number (0..1),\\n'\n",
        "            '  \"trace\": array of strings\\n'\n",
        "            \"}\\n\\n\"\n",
        "            \"Rules:\\n\"\n",
        "            \"- If Query is non-empty, use_semantic MUST be true.\\n\"\n",
        "            \"- If intent is 'search' and hybrid enforcement is enabled, use_cf MUST be true.\\n\"\n",
        "            \"- If both tools are used, weights MUST sum to 1.\\n\"\n",
        "            \"- Avoid setting weight_cf to 0 when hybrid enforcement is enabled.\\n\"\n",
        "        )\n",
        "\n",
        "        schema_hint = {\n",
        "            \"use_cf\": True,\n",
        "            \"use_semantic\": True,\n",
        "            \"weight_cf\": 0.4,\n",
        "            \"weight_semantic\": 0.6,\n",
        "            \"trace\": [],\n",
        "        }\n",
        "\n",
        "        res = self.client.generate_json(\n",
        "            system_prompt=system_prompt,\n",
        "            user_prompt=user_prompt,\n",
        "            force_json=True,\n",
        "            schema_hint=schema_hint,\n",
        "            max_retries=1,\n",
        "        )\n",
        "\n",
        "        obj = res[\"data\"] or {}\n",
        "        trace = list(obj.get(\"trace\", [])) if isinstance(obj.get(\"trace\"), list) else []\n",
        "\n",
        "        # Defaults\n",
        "        use_semantic = bool(obj.get(\"use_semantic\", bool(query)))\n",
        "        use_cf = bool(obj.get(\"use_cf\", True))\n",
        "\n",
        "        # Enforce query -> semantic\n",
        "        if query:\n",
        "            use_semantic = True\n",
        "\n",
        "        # Hybrid enforcement for search\n",
        "        if self.cfg.enforce_hybrid_for_search and intent == \"search\":\n",
        "            use_cf = True\n",
        "            trace.append(\"Hybrid enforcement: search requires CF + semantic\")\n",
        "\n",
        "        # Weights\n",
        "        w_cf = float(obj.get(\"weight_cf\", 0.4 if use_cf else 0.0))\n",
        "        w_sem = float(obj.get(\"weight_semantic\", 0.6 if use_semantic else 0.0))\n",
        "\n",
        "        if use_cf and use_semantic:\n",
        "            # Enforce minimum CF share if enabled\n",
        "            if self.cfg.enforce_hybrid_for_search and intent == \"search\":\n",
        "                w_cf = max(w_cf, float(self.cfg.min_cf_weight))\n",
        "                w_sem = max(0.0, 1.0 - w_cf)\n",
        "\n",
        "            # Normalize\n",
        "            s = max(w_cf + w_sem, 1e-6)\n",
        "            w_cf, w_sem = w_cf / s, w_sem / s\n",
        "\n",
        "        elif use_cf and not use_semantic:\n",
        "            w_cf, w_sem = 1.0, 0.0\n",
        "\n",
        "        elif use_semantic and not use_cf:\n",
        "            # If hybrid enforcement is on and intent is search, this should not happen\n",
        "            if self.cfg.enforce_hybrid_for_search and intent == \"search\":\n",
        "                use_cf = True\n",
        "                w_cf = float(self.cfg.min_cf_weight)\n",
        "                w_sem = 1.0 - w_cf\n",
        "                trace.append(\"Corrected: CF re-enabled for hybrid search\")\n",
        "            else:\n",
        "                w_cf, w_sem = 0.0, 1.0\n",
        "\n",
        "        else:\n",
        "            # Never allow neither\n",
        "            use_cf, use_semantic = True, bool(query)\n",
        "            w_cf, w_sem = (1.0, 0.0) if not query else (0.4, 0.6)\n",
        "            trace.append(\"Corrected: at least one tool required\")\n",
        "\n",
        "        trace.extend(res.get(\"trace\", []))\n",
        "        trace.append(\"openai_planner_v2\")\n",
        "\n",
        "        out = {\n",
        "            \"use_cf\": use_cf,\n",
        "            \"use_semantic\": use_semantic,\n",
        "            \"weight_cf\": float(w_cf),\n",
        "            \"weight_semantic\": float(w_sem),\n",
        "            \"trace\": trace,\n",
        "        }\n",
        "\n",
        "        self._log(f\"Plan: use_cf={use_cf}, use_semantic={use_semantic}, w_cf={out['weight_cf']:.2f}, w_sem={out['weight_semantic']:.2f}\")\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "Ti-2YBmMqLGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile agents/critic_agent.py\n",
        "from __future__ import annotations\n",
        "from typing import Any, Dict, List\n",
        "from config import POCConfig\n",
        "from agents.openai_client import OpenAIJSONClient\n",
        "\n",
        "\n",
        "class CriticAgent:\n",
        "    \"\"\"\n",
        "    Critic v2:\n",
        "    - Enforces hard safety / suitability constraints\n",
        "    - Evaluates novelty & popularity\n",
        "    - Checks genre diversity\n",
        "    - Can request advantage-weight + novelty adjustments\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: POCConfig):\n",
        "        self.cfg = cfg\n",
        "        self.client = OpenAIJSONClient(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0.2,\n",
        "            verbose=bool(cfg.verbose_openai),\n",
        "        )\n",
        "        self.verbose = bool(cfg.verbose)\n",
        "\n",
        "    def _log(self, msg: str):\n",
        "        if self.verbose:\n",
        "            print(f\"[CriticAgent] {msg}\")\n",
        "\n",
        "    def _genre_diversity(self, recs: List[Dict[str, Any]], topn: int) -> int:\n",
        "        genres = set()\n",
        "        for r in recs[:topn]:\n",
        "            g = (r.get(\"genres\") or \"\")\n",
        "            for token in g.split(\"|\"):\n",
        "                t = token.strip()\n",
        "                if t:\n",
        "                    genres.add(t)\n",
        "        return len(genres)\n",
        "\n",
        "    def _mean_popularity(self, recs: List[Dict[str, Any]], topn: int) -> float:\n",
        "        vals = []\n",
        "        for r in recs[:topn]:\n",
        "            pop = ((r.get(\"signals\") or {}).get(\"baseline_popularity\"))\n",
        "            if isinstance(pop, (int, float)):\n",
        "                vals.append(float(pop))\n",
        "        return float(sum(vals) / max(len(vals), 1))\n",
        "\n",
        "    def run(\n",
        "        self,\n",
        "        intent: str,\n",
        "        context: Dict[str, Any],\n",
        "        recs: List[Dict[str, Any]],\n",
        "    ) -> Dict[str, Any]:\n",
        "\n",
        "        intent = (intent or \"explore\").strip().lower()\n",
        "        topn = int(self.cfg.critic_topn)\n",
        "\n",
        "        needs_rerank = False\n",
        "        adjustments: Dict[str, Any] = {}\n",
        "        trace: List[str] = []\n",
        "\n",
        "        # 1) DETERMINISTIC GUARDRAILS\n",
        "\n",
        "        mean_pop = self._mean_popularity(recs, topn)\n",
        "        if mean_pop > float(self.cfg.popularity_mean_threshold):\n",
        "            needs_rerank = True\n",
        "            adjustments[\"novelty_lambda\"] = min(\n",
        "                0.35, float(self.cfg.novelty_lambda) + 0.10\n",
        "            )\n",
        "            trace.append(f\"Top-{topn} mean popularity too high: {mean_pop:.2f}\")\n",
        "\n",
        "        gdiv = self._genre_diversity(recs, topn)\n",
        "        if gdiv < int(self.cfg.genre_diversity_min_unique):\n",
        "            needs_rerank = True\n",
        "            adjustments[\"diversity_boost\"] = 0.10\n",
        "            trace.append(f\"Genre diversity too low: unique_genres={gdiv}\")\n",
        "\n",
        "        # ======================================================\n",
        "        # 2) HARD MATURITY / SUITABILITY VETO  (OPTION B)\n",
        "        # ======================================================\n",
        "\n",
        "        genres: List[str] = []\n",
        "        for r in recs[:topn]:\n",
        "            g = r.get(\"genres\", \"\")\n",
        "            if isinstance(g, str):\n",
        "                genres.extend([x.strip() for x in g.split(\"|\")])\n",
        "\n",
        "        children_ratio = (\n",
        "            sum(1 for g in genres if g.lower() == \"children\")\n",
        "            / max(1, len(genres))\n",
        "        )\n",
        "\n",
        "        user_query = (context.get(\"query\") or \"\").lower()\n",
        "        children_allowed = any(\n",
        "            kw in user_query\n",
        "            for kw in [\"kid\", \"kids\", \"family\", \"children\", \"child\"]\n",
        "        )\n",
        "\n",
        "        if children_ratio > 0.30 and not children_allowed:\n",
        "            needs_rerank = True\n",
        "            adjustments[\"exclude_genres\"] = [\"Children\"]\n",
        "            trace.append(\"hard_veto_children_content\")\n",
        "\n",
        "        # 3) LLM CRITIQUE\n",
        "\n",
        "        system_prompt = (\n",
        "            \"You are a recommendation critic for a movie recommender.\\n\"\n",
        "            \"Return ONLY valid JSON.\\n\"\n",
        "            \"Do NOT include commentary.\\n\"\n",
        "            \"Do NOT reveal hidden reasoning.\\n\"\n",
        "        )\n",
        "\n",
        "        user_prompt = (\n",
        "            f\"Intent: {intent}\\n\"\n",
        "            f\"User context: {context}\\n\\n\"\n",
        "            \"You will be given the top recommendations with optional signals.\\n\"\n",
        "            \"Assess:\\n\"\n",
        "            \"- Does the list satisfy the query and constraints?\\n\"\n",
        "            \"- Is novelty too low (too popular)?\\n\"\n",
        "            \"- Is the list too narrow in genres?\\n\"\n",
        "            \"- Are advantage signals reflected in top-ranked items?\\n\\n\"\n",
        "            \"Return JSON with exactly these keys:\\n\"\n",
        "            \"{\\n\"\n",
        "            '  \"needs_rerank\": boolean,\\n'\n",
        "            '  \"adjustments\": object,\\n'\n",
        "            '  \"trace\": array of strings\\n'\n",
        "            \"}\\n\\n\"\n",
        "            f\"Top recommendations:\\n{recs[:10]}\\n\"\n",
        "        )\n",
        "\n",
        "        schema_hint = {\n",
        "            \"needs_rerank\": needs_rerank,\n",
        "            \"adjustments\": adjustments,\n",
        "            \"trace\": trace,\n",
        "        }\n",
        "\n",
        "        res = self.client.generate_json(\n",
        "            system_prompt=system_prompt,\n",
        "            user_prompt=user_prompt,\n",
        "            force_json=True,\n",
        "            schema_hint=schema_hint,\n",
        "            max_retries=1,\n",
        "        )\n",
        "\n",
        "        data = res.get(\"data\") or {}\n",
        "        llm_needs = bool(data.get(\"needs_rerank\", False))\n",
        "        llm_adj = data.get(\"adjustments\", {})\n",
        "        llm_trace = data.get(\"trace\", [])\n",
        "\n",
        "        # 4) MERGE DECISIONS (HARD RULES WIN)\n",
        "\n",
        "        needs_rerank = bool(needs_rerank or llm_needs)\n",
        "        if isinstance(llm_adj, dict):\n",
        "            adjustments.update(llm_adj)\n",
        "\n",
        "        if isinstance(llm_trace, list):\n",
        "            trace.extend(llm_trace)\n",
        "\n",
        "        trace.extend(res.get(\"trace\", []))\n",
        "        trace.append(\"openai_critic_v2\")\n",
        "\n",
        "        out = {\n",
        "            \"needs_rerank\": needs_rerank,\n",
        "            \"adjustments\": adjustments,\n",
        "            \"trace\": trace,\n",
        "        }\n",
        "\n",
        "        self._log(\n",
        "            f\"Critic: needs_rerank={needs_rerank}, adjustments={adjustments}\"\n",
        "        )\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "jpJgw7BRqOPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile agents/explainer_agent.py\n",
        "import json\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from agents.openai_client import OpenAIJSONClient\n",
        "\n",
        "\n",
        "class ExplainerAgent:\n",
        "    \"\"\"\n",
        "    OpenAI-backed explanation agent.\n",
        "\n",
        "    Design goals:\n",
        "    - Conversational and user-facing\n",
        "    - Explicitly tied to the user's stated constraints\n",
        "    - Honest about strength of fit (no forced justification)\n",
        "    - Grounded in titles/genres only (no invented plot facts)\n",
        "    - No algorithm, model, or scoring language\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
        "        self.llm = OpenAIJSONClient(model=model, temperature=0.55)\n",
        "\n",
        "    def run(\n",
        "        self,\n",
        "        intent: str,\n",
        "        strategy: Dict[str, Any],\n",
        "        recs: List[Dict[str, Any]],\n",
        "        context: Optional[Dict[str, Any]] = None,\n",
        "    ) -> Dict[str, Any]:\n",
        "        context = context or {}\n",
        "\n",
        "        system_prompt = \"\"\"\n",
        "You are the Netflix Explainer.\n",
        "\n",
        "Your job is to explain WHY these recommendations fit the user's request —\n",
        "without exaggeration and without forcing relevance.\n",
        "\n",
        "CORE PRINCIPLES:\n",
        "- Be honest about fit quality. Not every recommendation is a perfect match.\n",
        "- Emphasize the strongest matches first.\n",
        "- If a title is only a partial or tonal match, say so gently.\n",
        "- Never invent plot details or specific scenes.\n",
        "\n",
        "RULES:\n",
        "- Speak directly TO the user (\"you\"), never AS the user.\n",
        "- Do NOT mention algorithms, models, embeddings, rankings, or scores.\n",
        "- Do NOT claim deep romance, realism, or emotional depth unless the title clearly supports it.\n",
        "- Use careful language when appropriate:\n",
        "  \"leans more toward\", \"has elements of\", \"is less about romance but shares a grounded tone\".\n",
        "- Explicitly reference the user's constraints (e.g., \"feels real\", \"not cheesy\", \"grounded\", \"emotionally honest\").\n",
        "- Avoid generic genre-only explanations.\n",
        "- Output ONLY valid JSON (no markdown, no commentary).\n",
        "\n",
        "TONE:\n",
        "- Calm, editorial, and human.\n",
        "- Similar to Netflix's in-app editorial blurbs.\n",
        "\"\"\"\n",
        "\n",
        "        # Keep explanation grounded: title + genres only\n",
        "        top3 = [\n",
        "            {\n",
        "                \"title\": r.get(\"title\", \"\"),\n",
        "                \"genres\": r.get(\"genres\", \"\"),\n",
        "            }\n",
        "            for r in (recs or [])[:3]\n",
        "        ]\n",
        "\n",
        "        user_prompt = f\"\"\"\n",
        "User intent:\n",
        "{intent}\n",
        "\n",
        "User context (JSON):\n",
        "{json.dumps(context, indent=2)}\n",
        "\n",
        "Top recommendations (title + genres only):\n",
        "{json.dumps(top3, indent=2)}\n",
        "\n",
        "WRITE:\n",
        "- one_liner:\n",
        "  ONE sentence summarizing why these picks were chosen,\n",
        "  explicitly referencing the user's constraints.\n",
        "\n",
        "- bullets:\n",
        "  EXACTLY 3 bullets.\n",
        "  Each bullet must:\n",
        "  - Mention a specific title.\n",
        "  - Explain how it aligns with the user's constraints.\n",
        "  - Use honest framing (strong match vs partial/tonal match).\n",
        "  - Avoid plot claims or factual specifics.\n",
        "\n",
        "IMPORTANT:\n",
        "- If a movie is not clearly romantic or emotionally grounded,\n",
        "  describe it as a tonal or adjacent match rather than a direct one.\n",
        "- Do NOT try to justify poor matches.\n",
        "\n",
        "RETURN ONLY THIS JSON SCHEMA:\n",
        "{{\n",
        "  \"one_liner\": \"string\",\n",
        "  \"bullets\": [\"string\", \"string\", \"string\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "        schema_hint = {\n",
        "            \"one_liner\": \"These films focus on grounded, emotionally honest storytelling rather than glossy or exaggerated romance.\",\n",
        "            \"bullets\": [\n",
        "                \"Get Real (1998) is often described as emotionally raw and understated, which aligns well with a preference for romance that feels genuine rather than performative.\",\n",
        "                \"All the Real Girls (2003) leans into quiet, character-driven moments, favoring emotional realism over dramatic spectacle.\",\n",
        "                \"Blue Valentine (2010) is known for its unvarnished tone, offering a more difficult but deeply authentic take on romantic relationships.\",\n",
        "            ],\n",
        "        }\n",
        "\n",
        "        resp = self.llm.generate_json(\n",
        "            system_prompt=system_prompt,\n",
        "            user_prompt=user_prompt,\n",
        "            force_json=True,\n",
        "            schema_hint=schema_hint,\n",
        "            max_retries=1,\n",
        "        )\n",
        "\n",
        "        data = resp[\"data\"] if resp[\"ok\"] else {}\n",
        "\n",
        "        one_liner = data.get(\"one_liner\", \"\")\n",
        "        bullets = data.get(\"bullets\", [])\n",
        "\n",
        "        if not isinstance(one_liner, str):\n",
        "            one_liner = \"\"\n",
        "\n",
        "        if not isinstance(bullets, list):\n",
        "            bullets = []\n",
        "\n",
        "        bullets = [b for b in bullets if isinstance(b, str)]\n",
        "\n",
        "        # Ensure exactly 3 bullets (safe padding)\n",
        "        while len(bullets) < 3:\n",
        "            bullets.append(\"\")\n",
        "        bullets = bullets[:3]\n",
        "\n",
        "        return {\n",
        "            \"one_liner\": one_liner,\n",
        "            \"bullets\": bullets,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "jvFpoWrkqQBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile graph/orchestrator.py\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import Dict, Any\n",
        "\n",
        "\n",
        "class AgenticGraph:\n",
        "    \"\"\"\n",
        "    LangGraph orchestrator (v1/v2 compatible).\n",
        "\n",
        "    Key points:\n",
        "    - uses intent_obj key\n",
        "    - trace_log is appended at each node\n",
        "    - supports critic-driven rerank adjustments\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, agents, tools, ranker, cfg):\n",
        "        self.agents = agents\n",
        "        self.tools = tools\n",
        "        self.ranker = ranker\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def build(self):\n",
        "        graph = StateGraph(dict)\n",
        "\n",
        "        def _append_trace(state: Dict[str, Any], event: Dict[str, Any]) -> Dict[str, Any]:\n",
        "            trace = list(state.get(\"trace_log\", []))\n",
        "            trace.append(event)\n",
        "            return {**state, \"trace_log\": trace}\n",
        "\n",
        "        def intent_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "            intent_obj = self.agents[\"intent\"].run(state[\"context\"])\n",
        "            state2 = {**state, \"intent_obj\": intent_obj}\n",
        "            return _append_trace(\n",
        "                state2,\n",
        "                {\n",
        "                    \"node\": \"intent\",\n",
        "                    \"intent\": intent_obj.get(\"intent\"),\n",
        "                    \"confidence\": intent_obj.get(\"confidence\"),\n",
        "                    \"trace\": intent_obj.get(\"trace\", []),\n",
        "                },\n",
        "            )\n",
        "\n",
        "        def plan_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "            intent = (state.get(\"intent_obj\") or {}).get(\"intent\", \"explore\")\n",
        "            plan = self.agents[\"planner\"].run(intent, state[\"context\"])\n",
        "            state2 = {**state, \"plan\": plan}\n",
        "            return _append_trace(\n",
        "                state2,\n",
        "                {\n",
        "                    \"node\": \"plan\",\n",
        "                    \"use_cf\": plan.get(\"use_cf\"),\n",
        "                    \"use_semantic\": plan.get(\"use_semantic\"),\n",
        "                    \"weights\": {\"cf\": plan.get(\"weight_cf\"), \"semantic\": plan.get(\"weight_semantic\")},\n",
        "                },\n",
        "            )\n",
        "\n",
        "        def retrieve_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "            updates = {}\n",
        "            plan = state.get(\"plan\") or {}\n",
        "            use_cf = bool(plan.get(\"use_cf\", True))\n",
        "            use_sem = bool(plan.get(\"use_semantic\", False))\n",
        "\n",
        "            if use_cf:\n",
        "                updates[\"cf\"] = self.tools[\"cf\"].recommend(state[\"user_id\"], self.cfg.cf_k)\n",
        "\n",
        "            if use_sem:\n",
        "                query = (state[\"context\"].get(\"query\") or \"\").strip()\n",
        "                if query:\n",
        "                    updates[\"sem\"] = self.tools[\"semantic\"].search(query, self.cfg.semantic_k)\n",
        "                else:\n",
        "                    updates[\"sem\"] = []\n",
        "\n",
        "            state2 = {**state, **updates}\n",
        "            return _append_trace(\n",
        "                state2,\n",
        "                {\n",
        "                    \"node\": \"retrieve\",\n",
        "                    \"use_cf\": use_cf,\n",
        "                    \"use_semantic\": use_sem,\n",
        "                    \"cf_rows\": int(len(state2.get(\"cf\"))) if state2.get(\"cf\") is not None else 0,\n",
        "                    \"sem_rows\": int(len(state2.get(\"sem\"))) if state2.get(\"sem\") is not None else 0,\n",
        "                },\n",
        "            )\n",
        "\n",
        "        def rank_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "            recs = self.ranker(state)\n",
        "            state2 = {**state, \"recs\": recs}\n",
        "            top = recs[0][\"title\"] if recs else None\n",
        "            return _append_trace(state2, {\"node\": \"rank\", \"top1\": top, \"recs_count\": len(recs)})\n",
        "\n",
        "        def critic_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "            intent = (state.get(\"intent_obj\") or {}).get(\"intent\", \"explore\")\n",
        "            critic = self.agents[\"critic\"].run(intent, state[\"context\"], state.get(\"recs\", []))\n",
        "            state2 = {**state, \"critic\": critic}\n",
        "            return _append_trace(\n",
        "                state2,\n",
        "                {\n",
        "                    \"node\": \"critic\",\n",
        "                    \"needs_rerank\": critic.get(\"needs_rerank\"),\n",
        "                    \"adjustments\": critic.get(\"adjustments\", {}),\n",
        "                },\n",
        "            )\n",
        "\n",
        "        def should_rerank(state: Dict[str, Any]) -> str:\n",
        "            critic = state.get(\"critic\") or {}\n",
        "            return \"rerank\" if bool(critic.get(\"needs_rerank\", False)) else \"explain\"\n",
        "\n",
        "        def rerank_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "            \"\"\"\n",
        "            Apply critic adjustments in-state and re-rank once.\n",
        "            We do NOT mutate global cfg; we only tweak state knobs.\n",
        "            \"\"\"\n",
        "            critic = state.get(\"critic\") or {}\n",
        "            adj = critic.get(\"adjustments\") or {}\n",
        "\n",
        "            # Store knobs in state for ranker_v2 to read if you want to extend;\n",
        "            # For now, rankers use cfg, so we adjust plan weights only (safe).\n",
        "            plan = dict(state.get(\"plan\") or {})\n",
        "            w_cf = float(plan.get(\"weight_cf\", 0.4))\n",
        "            w_sem = float(plan.get(\"weight_semantic\", 0.6))\n",
        "\n",
        "            w_cf_delta = float(adj.get(\"weight_cf_delta\", 0.0))\n",
        "            w_sem_delta = float(adj.get(\"weight_semantic_delta\", 0.0))\n",
        "\n",
        "            w_cf = max(0.0, min(1.0, w_cf + w_cf_delta))\n",
        "            w_sem = max(0.0, min(1.0, w_sem + w_sem_delta))\n",
        "\n",
        "            if w_cf > 0 and w_sem > 0:\n",
        "                s = max(w_cf + w_sem, 1e-6)\n",
        "                w_cf, w_sem = w_cf / s, w_sem / s\n",
        "            elif w_cf > 0:\n",
        "                w_cf, w_sem = 1.0, 0.0\n",
        "            else:\n",
        "                w_cf, w_sem = 0.0, 1.0\n",
        "\n",
        "            plan[\"weight_cf\"] = float(w_cf)\n",
        "            plan[\"weight_semantic\"] = float(w_sem)\n",
        "            plan_trace = list(plan.get(\"trace\", [])) if isinstance(plan.get(\"trace\", []), list) else []\n",
        "            plan_trace.append(\"critic_rerank_applied\")\n",
        "            plan[\"trace\"] = plan_trace\n",
        "\n",
        "            state2 = {**state, \"plan\": plan}\n",
        "\n",
        "            # Re-rank once\n",
        "            recs = self.ranker(state2)\n",
        "            state3 = {**state2, \"recs\": recs}\n",
        "\n",
        "            return _append_trace(\n",
        "                state3,\n",
        "                {\n",
        "                    \"node\": \"rerank\",\n",
        "                    \"applied_adjustments\": adj,\n",
        "                    \"new_weights\": {\"cf\": plan[\"weight_cf\"], \"semantic\": plan[\"weight_semantic\"]},\n",
        "                    \"top1\": recs[0][\"title\"] if recs else None,\n",
        "                },\n",
        "            )\n",
        "\n",
        "        def explain_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "            intent = (state.get(\"intent_obj\") or {}).get(\"intent\", \"explore\")\n",
        "            explanation = self.agents[\"explainer\"].run(intent, state.get(\"plan\", {}), state.get(\"recs\", []))\n",
        "            state2 = {**state, \"explanation\": explanation}\n",
        "            return _append_trace(state2, {\"node\": \"explain\", \"one_liner\": explanation.get(\"one_liner\", \"\")})\n",
        "\n",
        "        graph.add_node(\"intent\", intent_node)\n",
        "        graph.add_node(\"plan\", plan_node)\n",
        "        graph.add_node(\"retrieve\", retrieve_node)\n",
        "        graph.add_node(\"rank\", rank_node)\n",
        "        graph.add_node(\"critic\", critic_node)\n",
        "        graph.add_node(\"rerank\", rerank_node)\n",
        "        graph.add_node(\"explain\", explain_node)\n",
        "\n",
        "        graph.add_edge(START, \"intent\")\n",
        "        graph.add_edge(\"intent\", \"plan\")\n",
        "        graph.add_edge(\"plan\", \"retrieve\")\n",
        "        graph.add_edge(\"retrieve\", \"rank\")\n",
        "        graph.add_edge(\"rank\", \"critic\")\n",
        "        graph.add_conditional_edges(\"critic\", should_rerank, {\"rerank\": \"rerank\", \"explain\": \"explain\"})\n",
        "        graph.add_edge(\"rerank\", \"explain\")\n",
        "        graph.add_edge(\"explain\", END)\n",
        "\n",
        "        return graph.compile()\n"
      ],
      "metadata": {
        "id": "KpNf8QQOqVOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile agents/openai_client.py\n",
        "import json\n",
        "from typing import Any, Dict, Optional, Tuple\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "class OpenAIJSONClient:\n",
        "    \"\"\"\n",
        "    OpenAI helper with:\n",
        "    - JSON-only enforcement\n",
        "    - bounded repair\n",
        "    - structured verbosity (NO chain-of-thought)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str = \"gpt-4o-mini\",\n",
        "        temperature: float = 0.2,\n",
        "        timeout: Optional[float] = None,\n",
        "        verbose: bool = False,\n",
        "    ):\n",
        "        self.client = OpenAI()\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.timeout = timeout\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def _log(self, msg: str):\n",
        "        if self.verbose:\n",
        "            print(f\"[OpenAIJSONClient] {msg}\")\n",
        "\n",
        "    def _call(self, system_prompt: str, user_prompt: str, force_json: bool = True) -> str:\n",
        "        self._log(\"Calling OpenAI API\")\n",
        "\n",
        "        kwargs: Dict[str, Any] = {}\n",
        "        if force_json:\n",
        "            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
        "\n",
        "        resp = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            temperature=self.temperature,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "                {\"role\": \"user\", \"content\": user_prompt.strip()},\n",
        "            ],\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        raw = resp.choices[0].message.content.strip()\n",
        "        self._log(f\"Raw response length: {len(raw)} chars\")\n",
        "        return raw\n",
        "\n",
        "    @staticmethod\n",
        "    def _try_parse(raw: str) -> Tuple[Optional[Dict[str, Any]], bool]:\n",
        "        if not raw or not raw.strip():\n",
        "            return None, False\n",
        "\n",
        "        try:\n",
        "            return json.loads(raw), True\n",
        "        except Exception:\n",
        "            start = raw.find(\"{\")\n",
        "            end = raw.rfind(\"}\")\n",
        "            if start != -1 and end != -1 and end > start:\n",
        "                try:\n",
        "                    return json.loads(raw[start:end + 1]), True\n",
        "                except Exception:\n",
        "                    return None, False\n",
        "        return None, False\n",
        "\n",
        "    def generate_json(\n",
        "        self,\n",
        "        system_prompt: str,\n",
        "        user_prompt: str,\n",
        "        force_json: bool = True,\n",
        "        schema_hint: Optional[Dict[str, Any]] = None,\n",
        "        max_retries: int = 1,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "        {\n",
        "            \"ok\": bool,\n",
        "            \"data\": dict,\n",
        "            \"raw\": str,\n",
        "            \"trace\": list[str]\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        trace = []\n",
        "        raw = \"\"\n",
        "\n",
        "        # Attempt 1\n",
        "        try:\n",
        "            raw = self._call(system_prompt, user_prompt, force_json)\n",
        "            obj, ok = self._try_parse(raw)\n",
        "            if ok and isinstance(obj, dict):\n",
        "                self._log(\"JSON parsed successfully\")\n",
        "                return {\"ok\": True, \"data\": obj, \"raw\": raw, \"trace\": trace}\n",
        "            trace.append(\"json_parse_failed\")\n",
        "            self._log(\"JSON parse failed\")\n",
        "        except Exception as e:\n",
        "            trace.append(f\"openai_call_failed:{type(e).__name__}\")\n",
        "            self._log(f\"OpenAI call failed: {e}\")\n",
        "\n",
        "        # Repair attempt\n",
        "        for _ in range(max_retries):\n",
        "            repair_system = (\n",
        "                \"You repair invalid JSON.\\n\"\n",
        "                \"Return ONLY valid JSON.\\n\"\n",
        "                \"Do NOT include commentary.\\n\"\n",
        "            )\n",
        "\n",
        "            repair_user = (\n",
        "                \"The previous output was not valid JSON.\\n\"\n",
        "                \"Fix it and return ONLY JSON.\\n\"\n",
        "                \"If content is missing, use safe defaults.\\n\"\n",
        "            )\n",
        "\n",
        "            if schema_hint:\n",
        "                repair_user += f\"\\nJSON schema hint:\\n{json.dumps(schema_hint, indent=2)}\\n\"\n",
        "\n",
        "            repair_user += f\"\\nInvalid output:\\n{raw}\\n\"\n",
        "\n",
        "            try:\n",
        "                self._log(\"Attempting JSON repair\")\n",
        "                raw2 = self._call(repair_system, repair_user, force_json)\n",
        "                obj2, ok2 = self._try_parse(raw2)\n",
        "                if ok2 and isinstance(obj2, dict):\n",
        "                    trace.append(\"json_repair_success\")\n",
        "                    self._log(\"JSON repair succeeded\")\n",
        "                    return {\"ok\": True, \"data\": obj2, \"raw\": raw2, \"trace\": trace}\n",
        "                trace.append(\"json_repair_failed\")\n",
        "                raw = raw2\n",
        "            except Exception as e:\n",
        "                trace.append(f\"openai_repair_failed:{type(e).__name__}\")\n",
        "                self._log(f\"Repair failed: {e}\")\n",
        "\n",
        "        self._log(\"Falling back to safe empty JSON\")\n",
        "        return {\"ok\": False, \"data\": {}, \"raw\": raw, \"trace\": trace}\n"
      ],
      "metadata": {
        "id": "dAl8SWKWfDu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AGENTIC RECOMMENDER — FULL LOCAL DEMO\n",
        "import json\n",
        "import pandas as pd\n",
        "from config import POCConfig\n",
        "from tools.data_loader import MovieLensLoader\n",
        "from tools.cf_tool import SimpleCFRecommender\n",
        "from tools.semantic_tool import SemanticSearchTool\n",
        "from agents.intent_agent import IntentAgent\n",
        "from agents.planner_agent import PlannerAgent\n",
        "from agents.critic_agent import CriticAgent\n",
        "from agents.explainer_agent import ExplainerAgent\n",
        "from graph.orchestrator import AgenticGraph\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "# 1. CONFIG\n",
        "cfg = POCConfig()\n",
        "\n",
        "# 2. DATA LOADING (SAFE)\n",
        "print(\"Loading data...\")\n",
        "ratings, movies = MovieLensLoader(cfg).load()\n",
        "print(f\"Ratings: {len(ratings):,}, Movies: {len(movies):,}\")\n",
        "\n",
        "# 3. COLLABORATIVE FILTERING\n",
        "print(\"Training CF model...\")\n",
        "cf_tool = SimpleCFRecommender()\n",
        "cf_tool.fit(ratings)\n",
        "\n",
        "# 4. SEMANTIC SEARCH (PERSISTED)\n",
        "print(\"Preparing semantic index...\")\n",
        "semantic_tool = SemanticSearchTool(cfg)\n",
        "semantic_tool.build_or_load(movies)\n",
        "\n",
        "# 5. LOCAL LLM (CONTROL AGENTS)\n",
        "print(\"Loading local LLM...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(cfg.llm_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    cfg.llm_model_id,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "llm_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=cfg.llm_max_new_tokens,\n",
        "    temperature=cfg.llm_temperature,\n",
        ")\n",
        "\n",
        "class LocalLLM:\n",
        "    def generate_json(self, prompt: str):\n",
        "        out = llm_pipe(prompt, return_full_text=False)[0][\"generated_text\"]\n",
        "\n",
        "        if not out or not out.strip():\n",
        "            return {\n",
        "                \"parse_error\": True,\n",
        "                \"raw\": \"\",\n",
        "                \"trace\": [\"empty_llm_output\"]\n",
        "            }\n",
        "\n",
        "        # Try direct JSON\n",
        "        try:\n",
        "            return json.loads(out)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Try extracting first {...}\n",
        "        start = out.find(\"{\")\n",
        "        end = out.rfind(\"}\")\n",
        "\n",
        "        if start != -1 and end != -1 and end > start:\n",
        "            try:\n",
        "                return json.loads(out[start:end + 1])\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Final safe fallback\n",
        "        return {\n",
        "            \"parse_error\": True,\n",
        "            \"raw\": out,\n",
        "            \"trace\": [\"json_parse_failed\"]\n",
        "        }\n",
        "\n",
        "llm = LocalLLM()\n",
        "# 6. AGENTS\n",
        "intent_agent = IntentAgent()\n",
        "planner_agent = PlannerAgent(cfg)\n",
        "critic_agent = CriticAgent(cfg)\n",
        "explainer_agent = ExplainerAgent()\n",
        "\n",
        "agents = {\n",
        "    \"intent\": intent_agent,\n",
        "    \"planner\": planner_agent,\n",
        "    \"critic\": critic_agent,\n",
        "    \"explainer\": explainer_agent,\n",
        "}\n",
        "\n",
        "tools = {\n",
        "    \"cf\": cf_tool,\n",
        "    \"semantic\": semantic_tool,\n",
        "}\n",
        "\n",
        "# 7. RANKER (v1 / v2 SWITCH)\n",
        "from tools.item_stats import ItemStats\n",
        "from rankers.ranker_v1 import RankerV1\n",
        "from rankers.ranker_v2 import RankerV2\n",
        "\n",
        "movie_map = movies.set_index(\"movieId\")[[\"title\", \"genres\"]].to_dict(\"index\")\n",
        "\n",
        "# Build baseline statistics ONCE\n",
        "item_stats = ItemStats.build(ratings)\n",
        "\n",
        "if getattr(cfg, \"strategy_version\", \"v2\") == \"v1\":\n",
        "    print(\"Using Ranker v1 (baseline)\")\n",
        "    ranker = RankerV1(cfg, movie_map)\n",
        "else:\n",
        "    print(\"Using Ranker v2 (hybrid + advantage-weighted)\")\n",
        "    ranker = RankerV2(cfg, movie_map, item_stats)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 8. BUILD GRAPH\n",
        "# ---------------------------\n",
        "graph = AgenticGraph(\n",
        "    agents=agents,\n",
        "    tools=tools,\n",
        "    ranker=ranker,\n",
        "    cfg=cfg\n",
        ").build()\n",
        "\n",
        "# ---------------------------\n",
        "# 9. RUN DEMO QUERY\n",
        "# ---------------------------\n",
        "demo_input = {\n",
        "    \"user_id\": 7,\n",
        "    \"context\": {\n",
        "        \"query\": \"romantic movie that feels real, not cheesy\",\n",
        "        \"available_minutes\": 120,\n",
        "        \"novelty_tolerance\": 0.4\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nRUNNING AGENTIC RECOMMENDATION...\\n\")\n",
        "print(\"TRACE: invoking graph with keys:\", list(demo_input.keys()))\n",
        "print(\"TRACE: demo_input =\", demo_input)\n",
        "result = graph.invoke(demo_input)\n",
        "print(\"TRACE: result keys:\", list(result.keys()))\n",
        "\n",
        "# ---------------------------\n",
        "# 10. DISPLAY RESULTS\n",
        "# ---------------------------\n",
        "print(\"INTENT\")\n",
        "print(json.dumps(result[\"intent_obj\"], indent=2))\n",
        "\n",
        "print(\"\\nSTRATEGY\")\n",
        "print(json.dumps(result[\"plan\"], indent=2))\n",
        "\n",
        "print(\"\\nTOP RECOMMENDATIONS\")\n",
        "for i, r in enumerate(result[\"recs\"], 1):\n",
        "    print(f\"{i:>2}. {r['title']}  ({r['genres']})\")\n",
        "\n",
        "print(\"\\nEXPLANATION\")\n",
        "print(json.dumps(result[\"explanation\"], indent=2))\n",
        "\n",
        "print(\"\\nAGENT TRACE (ABRIDGED)\")\n",
        "print(\"\\nTRACE LOG (per node)\")\n",
        "print(json.dumps(result.get(\"trace_log\", []), indent=2))\n",
        "\n",
        "print(\"\\nINTENT\")\n",
        "print(json.dumps(result[\"intent_obj\"], indent=2))\n",
        "\n",
        "print(\"\\nPLAN\")\n",
        "print(json.dumps(result[\"plan\"], indent=2))\n",
        "\n",
        "print(\"\\nCRITIC\")\n",
        "print(json.dumps(result[\"critic\"], indent=2))"
      ],
      "metadata": {
        "id": "ePtdVGuIqXiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"embeddings/movie_embeddings.npy\")\n",
        "files.download(\"embeddings/movie_ids.json\")"
      ],
      "metadata": {
        "id": "eHoTAnV2lyZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "34FkXDo7dsRr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}